import argparse
import os
import sqlite3
from pathlib import Path
from datetime import datetime, timedelta

from llama_index.core import VectorStoreIndex, get_response_synthesizer
from llama_index.core.settings import Settings
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.core.schema import Document
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.prompts import PromptTemplate
from llama_index.core.storage import StorageContext
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from pydantic import Field
import numpy as np
import chromadb

# ===== å®šæ•° =====
OLLAMA_MODEL = "llama3.1:8b"
CHROMA_DIR = "chroma_store"
HISTORY_DB = "./docs/history.db"
PDFDB_DB = "./pdfdb/documents.db"
CHUNK_SIZE = 256
CHUNK_OVERLAP = 20

# ===== LLM / Embedding è¨­å®š =====
llm = Ollama(model=OLLAMA_MODEL, request_timeout=120.0)
embed_model = OllamaEmbedding(model_name=OLLAMA_MODEL, request_timeout=60.0)
Settings.llm = llm
Settings.embed_model = embed_model

# ===== Chroma ã®è¨­å®š =====
chroma_client = chromadb.PersistentClient(path=CHROMA_DIR)
chroma_collection = chroma_client.get_or_create_collection("rag_docs")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# ===== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ =====
qa_template = PromptTemplate(
    """\
æ–‡è„ˆæƒ…å ±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:
---------------------
{context_str}
---------------------

ã“ã®æ–‡è„ˆæƒ…å ±ã‚’ã‚‚ã¨ã«ã€ä»¥ä¸‹ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
è³ªå•: {query_str}

å›ç­”: """
)

# ===== Softmaxé¡ä¼¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ =====
class MySoftmaxPostprocessor(BaseNodePostprocessor):
    temperature: float = Field(default=1.0)
    top_k: int = Field(default=None)
    similarity_cutoff: float = Field(default=None)

    def _postprocess_nodes(self, nodes_with_scores, query_str=None):
        if not nodes_with_scores:
            return []

        scores = np.array([n.score for n in nodes_with_scores])
        scores = scores / self.temperature
        softmax_scores = np.exp(scores) / np.exp(scores).sum()

        for node, score in zip(nodes_with_scores, softmax_scores):
            node.score = score

        if self.similarity_cutoff is not None:
            nodes_with_scores = [n for n in nodes_with_scores if n.score >= self.similarity_cutoff]

        if self.top_k is not None:
            nodes_with_scores = sorted(nodes_with_scores, key=lambda n: n.score, reverse=True)
            nodes_with_scores = nodes_with_scores[:self.top_k]

        return nodes_with_scores

# ===== PDFç”¨: documentsãƒ†ãƒ¼ãƒ–ãƒ«åˆæœŸåŒ– =====
def init_pdfdb(path=PDFDB_DB):
    Path(os.path.dirname(path)).mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(path)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS documents (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            full_text TEXT,
            summary TEXT,
            created_at TIMESTAMP
        )
    ''')
    conn.commit()
    conn.close()

# ===== å±¥æ­´å–å¾— =====
def get_history_documents(path, days=7, limit=50):
    if not os.path.exists(path):
        print(f"âš ï¸ å±¥æ­´DBãŒå­˜åœ¨ã—ã¾ã›ã‚“: {path}")
        return []

    chrome_epoch = datetime(1601, 1, 1)
    cutoff = datetime.utcnow() - timedelta(days=days)
    cutoff_microseconds = int((cutoff - chrome_epoch).total_seconds() * 1_000_000)

    conn = sqlite3.connect(path)
    cursor = conn.cursor()
    cursor.execute("SELECT title, url, visit_time FROM history ORDER BY visit_time DESC LIMIT ?", (limit,))
    docs = [Document(text=f"{title or '(no title)'}\n{url}") for title, url, _ in cursor.fetchall()]
    conn.close()
    return docs

# ===== PDFå–å¾— =====
def get_pdf_documents(path, limit=50):
    if not os.path.exists(path):
        print(f"âš ï¸ PDF DBãŒå­˜åœ¨ã—ã¾ã›ã‚“: {path}")
        return []

    conn = sqlite3.connect(path)
    cursor = conn.cursor()
    cursor.execute("SELECT title, full_text, summary, created_at FROM documents ORDER BY created_at DESC LIMIT ?", (limit,))
    docs = [
        Document(text=f"{title}\n{summary}\n\n{full_text}")
        for title, full_text, summary, _ in cursor.fetchall()
    ]
    conn.close()
    return docs

# ===== ãƒ¡ã‚¤ãƒ³å‡¦ç† =====
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--source", choices=["history", "pdf"], required=True, help="èª­ã¿è¾¼ã¿å…ƒã‚’é¸æŠï¼ˆhistory or pdfï¼‰")
    args = parser.parse_args()

    if args.source == "pdf":
        init_pdfdb()
        docs = get_pdf_documents(PDFDB_DB)

    else:
        docs = get_history_documents(HISTORY_DB)

    if not docs:
        print("âš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        return

    parser_ = SimpleNodeParser.from_defaults(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    nodes = parser_.get_nodes_from_documents(docs)
    index = VectorStoreIndex(nodes, storage_context=storage_context)

    postprocessor = MySoftmaxPostprocessor(temperature=0.7, top_k=5)
    query_engine = index.as_query_engine(
        similarity_top_k=10,
        response_synthesizer=get_response_synthesizer(
            response_mode="tree_summarize",
            text_qa_template=qa_template,
        ),
        node_postprocessors=[postprocessor],
    )

    while True:
        query = input("\nğŸ” è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆqã§çµ‚äº†ï¼‰: ")
        if query.strip().lower() == "q":
            break
        print("å›ç­”ç”Ÿæˆä¸­...")
        response = query_engine.query(query)
        print(f"\nğŸ§  å›ç­”:\n{response}")

# ===== å®Ÿè¡Œ =====
if __name__ == "__main__":
    main()


