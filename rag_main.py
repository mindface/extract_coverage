import os
from pathlib import Path
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.settings import Settings
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama
from llama_index.core.storage import StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.node_parser import SimpleNodeParser
import chromadb
from llama_index.core import get_response_synthesizer
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.prompts import PromptTemplate
import time
from datetime import datetime, timedelta
import sqlite3
from llama_index.core.schema import Document
import requests
from bs4 import BeautifulSoup
# === 1. ç’°å¢ƒè¨­å®š ===

OLLAMA_MODEL = "7shi/tanuki-dpo-v1.0"
DOCS_DIR = "docs"
CHROMA_DIR = "chroma_store"
CHUNK_SIZE = 256
CHUNK_OVERLAP = 20


Path(DOCS_DIR).mkdir(exist_ok=True)

def get_page_content(url: str) -> str:
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
        }
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        return soup.get_text(separator="\n", strip=True)
    except Exception as e:
        print(f"[WARN] ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {url} ({e})")
        return ""

def get_history_documents_from_txt(file_path: str = "docs/history.txt") -> list[Document]:
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    entries = content.strip().split("\n\n")
    return [Document(text=entry) for entry in entries]

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚’è¿½åŠ ï¼‰
llm = Ollama(model=OLLAMA_MODEL, request_timeout=120.0)
embed_model = OllamaEmbedding(
    model_name=OLLAMA_MODEL,
    request_timeout=60.0,
    embed_batch_size=4,
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
Settings.llm = llm
Settings.embed_model = embed_model

# ChromaDBã®è¨­å®š
chroma_client = chromadb.PersistentClient(path=CHROMA_DIR)
chroma_collection = chroma_client.get_or_create_collection("rag_docs")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

qa_template = PromptTemplate(
    """\
æ–‡è„ˆæƒ…å ±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:
---------------------
{context_str}
---------------------

ã“ã®æ–‡è„ˆæƒ…å ±ã‚’ã‚‚ã¨ã«ã€ä»¥ä¸‹ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
è³ªå•å†…å®¹ã¨é–¢ä¿‚ã®ãªã„æƒ…å ±ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
ã‚‚ã—æ–‡è„ˆæƒ…å ±ã‹ã‚‰å›ç­”ã§ããªã„å ´åˆã¯ã€Œãã®æƒ…å ±ã¯æ–‡è„ˆã‹ã‚‰è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: {query_str}

å›ç­”: """
)

print(f"[DEBUG] æ¥ç¶šä¸­ã®DBãƒ•ã‚¡ã‚¤ãƒ«: {os.path.abspath('./docs/history.db')}")
def get_history_documents_from_sqlite(
    path="./docs/history.db",
    days=7,
    limit=100,
    url_filter_keyword=None  # ç‰¹å®šæ–‡å­—åˆ—ã§URLã‚’ãƒ•ã‚£ãƒ«ã‚¿
):
    chrome_epoch = datetime(1601, 1, 1)
    cutoff = datetime.utcnow() - timedelta(days=days)
    cutoff_microseconds = int((cutoff - chrome_epoch).total_seconds() * 1_000_000)

    conn = sqlite3.connect(path)
    cursor = conn.cursor()
    # cursor.execute("""
    #     SELECT url, title FROM urls
    #     WHERE last_visit_time > ?
    #     ORDER BY last_visit_time DESC
    #     LIMIT ?
    # """, (cutoff_microseconds, limit))
    print(f"[DEBUG] å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    cursor.execute("""
        SELECT title, url, visit_time FROM history
        ORDER BY visit_time DESC LIMIT ?
        """, (limit,))
    print(f"[DEBUG] å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­... å®Œäº†")

    docs = []
    for title, url, visit_time in cursor.fetchall():
        if url_filter_keyword and url_filter_keyword not in get_page_content(url):
            continue

        text = f"{title or '(no title)'}\n{url}"
        docs.append(Document(text=text))

    conn.close()
    return docs

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã¨ãƒãƒ¼ãƒ‰ä½œæˆ
if len(os.listdir(DOCS_DIR)) == 0:
    print(f"è­¦å‘Š: {DOCS_DIR} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
    with open(f"{DOCS_DIR}/sample.txt", "w") as f:
        f.write("ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆç”¨ã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚")

print("ãƒ–ãƒ©ã‚¦ã‚¶å±¥æ­´ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆä¸­...")
history_docs = get_history_documents_from_sqlite(
      path="./docs/history.db",
      days=7,
      limit=100,
      url_filter_keyword="æ©Ÿæ¢°å­¦ç¿’"
    )

parser = SimpleNodeParser.from_defaults(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
nodes = parser.get_nodes_from_documents(history_docs)
if not history_docs:
    print("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹å±¥æ­´ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
    exit()


# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
try:
    print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆä¸­...")
    start_time = time.time()

    # ãƒãƒƒãƒå‡¦ç†ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
    batch_size = 10
    for i in range(0, len(nodes), batch_size):
        print(f"ãƒãƒƒãƒå‡¦ç†ä¸­: {i}/{len(nodes)} ãƒãƒ¼ãƒ‰")
        current_batch = nodes[i:i+batch_size]
        if i == 0:
            index = VectorStoreIndex(current_batch, storage_context=storage_context)
        else:
            index.insert_nodes(current_batch)

    end_time = time.time()
    print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸ (æ‰€è¦æ™‚é–“: {end_time - start_time:.2f}ç§’)")
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹åˆæˆå™¨ã®è¨­å®š
    response_synthesizer = get_response_synthesizer(
        response_mode="compact",
        text_qa_template=qa_template,
    )

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹åˆæˆå™¨ã®è¨­å®š
    response_synthesizer = get_response_synthesizer(
        response_mode="compact",
        text_qa_template=qa_template,
    )

    # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
    query_engine = index.as_query_engine(
        similarity_top_k=2,
        response_synthesizer=response_synthesizer,
        node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],
    )

    # å¯¾è©±ãƒ«ãƒ¼ãƒ—
    while True:
        question = input("\nğŸ” è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ q ã‚’å…¥åŠ›ï¼‰: ")
        if question.strip().lower() == "q":
            break
            
        print("å›ç­”ã‚’ç”Ÿæˆä¸­...")
        try:
            start_time = time.time()
            response = query_engine.query(question)
            end_time = time.time()
            print(f"\nğŸ§  å›ç­”:\n{response}")
            print(f"(æ‰€è¦æ™‚é–“: {end_time - start_time:.2f}ç§’)")
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            print("ã‚‚ã†ä¸€åº¦è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")

except Exception as e:
    print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
